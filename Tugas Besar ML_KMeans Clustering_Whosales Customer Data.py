# -*- coding: utf-8 -*-
"""Kmeans_2cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e8qe4Gdk1PeBIs7f5HvC6gZSgDBLs5PT

# **Tentang Dataset**

* Dataset yang digunakan dalam penelitian ini adalah Wholesale Customers Data Set, yang berasal dari UCI Machine Learning Repository https://archive.ics.uci.edu/dataset/292/wholesale+customers. Dataset ini banyak digunakan dalam studi mengenai segmentasi pelanggan dan analisis perilaku pembelian.
* Dataset ini berisi data pengeluaran tahunan pelanggan grosir pada berbagai kategori produk. Tujuan utamanya adalah menyediakan informasi yang dapat digunakan untuk mengidentifikasi pola pembelian dan mengelompokkan pelanggan berdasarkan perilaku mereka.

# **Latar Belakang**

Dalam dunia bisnis grosir, setiap pelanggan memiliki pola pengeluaran yang berbeda tergantung pada kebutuhan dan jenis usahanya. Tanpa segmentasi yang jelas, perusahaan sulit menyusun strategi pemasaran yang tepat dan efisien. Untuk mengatasi hal tersebut, diperlukan pendekatan analisis data yang mampu mengelompokkan pelanggan berdasarkan kesamaan perilaku pembelian mereka.

Salah satu metode yang dapat digunakan adalah KMeans Clustering, yaitu teknik machine learning yang dapat membagi pelanggan ke dalam beberapa kelompok berdasarkan pola pengeluaran tahunan mereka pada berbagai kategori produk seperti makanan segar, susu, grocery, dan lainnya. Dengan segmentasi ini, perusahaan dapat memahami kebutuhan setiap kelompok pelanggan dan merancang strategi bisnis yang lebih efektif.

# **Formulasi Masalah**

Perusahaan grosir menjual berbagai jenis produk kepada beragam jenis pelanggan, seperti retail kecil dan bisnis Horeca (Hotel, Restoran, dan Kafe). Setiap pelanggan memiliki pola pengeluaran yang berbeda terhadap kategori produk seperti makanan segar, susu, makanan beku, grocery, dan perlengkapan kebersihan.

Namun, perusahaan belum memiliki sistem segmentasi pelanggan yang jelas. Seluruh pelanggan diperlakukan secara seragam tanpa mempertimbangkan karakteristik belanja mereka. Akibatnya:



1.   Strategi pemasaran tidak tertarget dan berisiko tidak efektif.


2.   Pengelolaan stok barang tidak optimal karena tidak tahu produk mana yang dominan dibutuhkan oleh segmen tertentu.

3.   Peluang untuk meningkatkan loyalitas pelanggan dan efisiensi operasional menjadi terhambat.

# **Permasalahan Diselesaikan**

Bagaimana mengelompokkan pelanggan grosir berdasarkan pola pengeluaran tahunan mereka agar perusahaan dapat menyusun strategi pemasaran, logistik, dan layanan pelanggan yang lebih efektif dan tepat sasaran?
"""

import pandas as pd
import numpy as np
import warnings
from google.colab import drive

"""# **Eksplorasi dan Persiapan Data**

* Dataset terdiri dari 440 baris data dan 8 kolom fitur, masing-masing merepresentasikan informasi pelanggan ritel.
"""

df = pd.read_csv('/content/Wholesale customers data.csv')
df

"""* Hasil menunjukkan bahwa seluruh kolom bertipe int64, dan tidak terdapat missing value pada dataset (semua baris terisi lengkap)."""

print(df.shape)
print(df.info())
print(df.describe())
print(df.isnull().sum())
print(df.duplicated().sum())
# print(df.corr())

"""* Histogram Melihat apakah distribusi fitur numerik simetris atau miring (skewed), karena fitur yang sangat skewed dapat memengaruhi KMeans.
* Seluruh fitur menunjukkan distribusi yang condong ke kanan (right-skewed) yang artinya Sebagian besar data berada di nilai rendah, ini mengindikasikan seluruh fitur memiliki distribusi tidak simetris dan adanya outlier atau pelanggan ekstrem dengan pembelian sangat besar. Perlu dilakukan normalisasi atau transformasi skala agar fitur lebih seimbang.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# fitur pengeluaran
pengeluaran_cols = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
plt.figure(figsize=(14, 10))

# Histogram
for i, col in enumerate(pengeluaran_cols, 1):
    plt.subplot(3, 2, i)
    sns.histplot(df[col], kde=True, bins=30, color='mediumseagreen')
    plt.title(f'Distribusi {col}', fontsize=12)
    plt.xlabel(col)
    plt.ylabel('Frekuensi')

plt.tight_layout()
plt.show()

# sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
# plt.title('Korelasi antar Fitur')
# plt.show()

"""# **Scaling Data**

KMeans Clustering menggunakan jarak (Euclidean distance) sebagai dasar pengelompokan. Jika data tidak dinormalisasi, fitur dengan skala besar hasil cluster bisa bias terhadap fitur tertentu dan tidak merefleksikan hubungan sesungguhnya antar data

Tujuan Normalisasi Membuat semua fitur berada dalam skala yang sama, biasanya dalam rentang [0, 1].
"""

x = df.iloc[:, [2,3,4,5,6,7]]
y = (x - x.min()) / (x.max() - x.min())
y

data = y.values.tolist()

"""* menyederhanakan struktur data agar mudah diproses dengan operasi dasar Python. Jika masih berupa DataFrame (pandas), maka perhitungan tidak akan jalan karena elemen-elemen DataFrame bukan float/int langsung.

# Pemodelan: KMeans Clustering (Tanpa Library)

1. Inisialisasi k titik pusat secara acak.

2. Hitung jarak Euclidean antara titik data dan tiap centroid.

3. Kelompokkan titik ke cluster terdekat.

4. Update centroid sebagai rata-rata dari anggota cluster.

5. Ulangi langkah 2â€“4 sampai centroid stabil (konvergen).
"""

import random
import math

# inisialisasi cluster awal
k = 2
max_iter = 100

# jarak Euclidean
def euclidean(p1, p2):
    return math.sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))

# Inisialisasi centroid acak
random.seed(42)
centroids = random.sample(data, k)

for iteration in range(max_iter):
    clusters = [[] for _ in range(k)]

    for point in data:
        distances = [euclidean(point, centroid) for centroid in centroids]
        cluster_idx = distances.index(min(distances))
        clusters[cluster_idx].append(point)

    # menyimpan centroid lama
    old_centroids = centroids.copy()

    # mengupdate centroid
    new_centroids = []
    for cluster in clusters:
        if cluster:
            new_centroid = [sum(dim) / len(cluster) for dim in zip(*cluster)]
        else:
            new_centroid = random.choice(data)  #cluster kosong
        new_centroids.append(new_centroid)

    centroids = new_centroids

    # menampilkan iterasi
    print(f"\nIterasi ke-{iteration + 1}")
    for i, centroid in enumerate(centroids):
        print(f"  Centroid {i+1}: {centroid}")
    for i, cluster in enumerate(clusters):
        print(f"  Cluster {i+1}: {len(cluster)} data")

    # cek konvergensi
    if all(euclidean(c1, c2) < 1e-4 for c1, c2 in zip(old_centroids, centroids)):
        print(f"\n Konvergen pada iterasi ke-{iteration+1}")
        break

"""# Evaluasi: Sum of Squared Error (SSE)

mengukur jarak kuadrat antara setiap titik data dalam cluster terhadap centroid cluster-nya. SSE menunjukkan seberapa kompak data dalam setiap cluster. Semakin kecil SSE, semakin baik (anggota cluster mendekati centroid).
"""

from sklearn.cluster import KMeans
wcs=[]
for i in range (1,11):
  kmeans=KMeans(n_clusters=i, init='k-means++', random_state=39)
  kmeans.fit(y)
  wcs.append(kmeans.inertia_)
plt.plot(range(1,11),wcs)
plt.title('Elbow Method')
plt.xlabel('k')
plt.ylabel('Withn Cluster Sum of Square')
#plt.savefig("elbow.pdf", format="pdf", bbox_inches="tight")
plt.show()

"""* Menggunaan Library Kmeans khusus visualisasi Elbow Method untuk Melihat  titik tekuk (elbow) di k = 2
* Grafik Elbow menunjukkan penurunan drastis hingga k=2 atau k=3 lalu datar, artinya k = 2 adalah titik optimal dari segi efisiensi dan performa.
"""

import math

#jarak euclidean
def euclidean(p1, p2):
    return math.sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))

# menghitung SSE dari cluster
def compute_sse(clusters, centroids):
    sse = 0
    for idx, cluster in enumerate(clusters):
        for point in cluster:
            sse += euclidean(point, centroids[idx]) ** 2
    return sse

# hasil
sse = compute_sse(clusters, centroids)
print("SSE:", round(sse, 4))

# fungsi euclidean
def euclidean(p1, p2):
    return math.sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))

# silhouette score
def silhouette_score_point(point, own_cluster, other_clusters):
    if len(own_cluster) <= 1:
        return 0

    # menghitung a(i)
    a = sum(euclidean(point, other) for other in own_cluster if other != point) / (len(own_cluster) - 1)

    # menghitung b(i)
    b_values = []
    for cluster in other_clusters:
        if len(cluster) == 0:
            continue
        b = sum(euclidean(point, other) for other in cluster) / len(cluster)
        b_values.append(b)
    b = min(b_values)

    # menghitung silhouette score
    s = (b - a) / max(a, b)
    return s
def compute_silhouette_score(clusters):
    silhouette_scores = []
    for idx, cluster in enumerate(clusters):
        for point in cluster:
            other_clusters = [c for i, c in enumerate(clusters) if i != idx]
            score = silhouette_score_point(point, cluster, other_clusters)
            silhouette_scores.append(score)

    overall_score = sum(silhouette_scores) / len(silhouette_scores)
    return overall_score

# hasil
score = compute_silhouette_score(clusters)
print("Silhouette Score:", round(score, 4))

"""* Nilai SSE 17.246 sudah cukup kecil untuk k=2. Digabung dengan silhouette score yang tinggi. Model clustering dengan k=2 adalah pilihan terbaik untuk data ini dari sisi pemisahan (silhouette) dan SSE

# Merge Data
"""

# Membuat list label
labels = [None] * len(data)
for cluster_index, cluster in enumerate(clusters):
    for point in cluster:
        index = data.index(point)
        labels[index] = cluster_index

# menambahkan kolom cluster ke df
labels = [l + 1 for l in labels]
df['cluster'] = labels
df.head()

df['cluster'].unique()

"""# Menggabungkan Cluster Dengan Scaling Data"""

df_cluster = df['cluster']

df.head()

df_channelreg = df[['Channel','Region']]

df_normcluster = pd.concat([df_channelreg,y, df_cluster], axis=1)
df_normcluster

"""#  Visualisasi K-Means

* Data Awal Sebelum Normalisasi
"""

import matplotlib.pyplot as plt
import itertools

# fitur pengeluaran
features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
colors = ['red', 'green', 'blue', 'purple', 'orange']

# scatter plot
pairs = list(itertools.combinations(features, 2))
plt.figure(figsize=(15, 15))

for i, (x_feat, y_feat) in enumerate(pairs):
    plt.subplot(5, 3, i+1)
    for cluster_id in sorted(df['cluster'].unique()):
        cluster_data = df[df['cluster'] == cluster_id]
        plt.scatter(cluster_data[x_feat], cluster_data[y_feat],
                    color=colors[cluster_id], label=f'Cluster {cluster_id}', alpha=0.5)
    plt.xlabel(x_feat)
    plt.ylabel(y_feat)
    plt.title(f'{x_feat} vs {y_feat}')
    plt.tight_layout()

plt.legend()
plt.suptitle("Scatter Plot dari Semua Pasangan Fitur Berdasarkan Cluster", fontsize=16, y=1.02)
plt.show()

"""* Data Setelah Normalisasi

"""

import matplotlib.pyplot as plt
import itertools

# fitur pengeluaran
features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
colors = ['red', 'green', 'blue', 'purple', 'orange']

# scatter plot
pairs = list(itertools.combinations(features, 2))
plt.figure(figsize=(15, 15))

for i, (x_feat, y_feat) in enumerate(pairs):
    plt.subplot(5, 3, i+1)
    for cluster_id in sorted(df_normcluster['cluster'].unique()):
        cluster_data = df_normcluster[df_normcluster['cluster'] == cluster_id]
        plt.scatter(cluster_data[x_feat], cluster_data[y_feat],
                    color=colors[cluster_id], label=f'Cluster {cluster_id}', alpha=0.5)
    plt.xlabel(x_feat)
    plt.ylabel(y_feat)
    plt.title(f'{x_feat} vs {y_feat}')
    plt.tight_layout()

plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.suptitle("Scatter Plot dari Semua Pasangan Fitur Berdasarkan Cluster (Data Ternormalisasi)", fontsize=16, y=1.02)
plt.show()

print(df_normcluster['cluster'].value_counts())

import matplotlib.pyplot as plt
import itertools

# fitur pengeluaran
features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
pairs = list(itertools.combinations(features, 2))

# mengambil semua label unik
unique_clusters = sorted(df_normcluster['cluster'].unique())
colors = plt.cm.get_cmap('Set1', len(unique_clusters))  # Colormap dinamis

plt.figure(figsize=(15, 15))

for i, (x_feat, y_feat) in enumerate(pairs):
    plt.subplot(5, 3, i+1)
    for idx, cluster_id in enumerate(unique_clusters):
        cluster_data = df_normcluster[df_normcluster['cluster'] == cluster_id]
        plt.scatter(cluster_data[x_feat], cluster_data[y_feat],
                    color=colors(idx), label=f'Cluster {cluster_id}', alpha=0.5)

        # menambahkan centroid
        x_idx = features.index(x_feat)
        y_idx = features.index(y_feat)
        if cluster_id - 1 < len(centroids):
            plt.scatter(centroids[cluster_id - 1][x_idx], centroids[cluster_id - 1][y_idx],
                        color='black', marker='X', s=100, edgecolor='white', linewidth=1.5)

    plt.xlabel(x_feat)
    plt.ylabel(y_feat)
    plt.title(f'{x_feat} vs {y_feat}')
    plt.tight_layout()

plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.suptitle("Scatter Plot dari Semua Pasangan Fitur dengan Centroid (Data Ternormalisasi)", fontsize=16, y=1.02)
plt.show()

sns.histplot(data=df, x='cluster', multiple="dodge", shrink=.8)

import seaborn as sns
import matplotlib.pyplot as plt

features = ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper', 'Delicassen']
plt.figure(figsize=(15, 12))

for i, feature in enumerate(features):
    plt.subplot(2, 3, i + 1)
    sns.boxplot(x='cluster', y=feature, data=df_normcluster, palette='Set2')
    plt.title(f'{feature} per Cluster')
    plt.xlabel('Cluster')
    plt.ylabel('Normalized Value')

plt.tight_layout()
plt.suptitle('Boxplot Setiap Fitur Pengeluaran Berdasarkan Cluster', fontsize=16, y=1.03)
plt.show()

"""# Mengubah Channel dan Region Sesuai Keterangan Kategorikal"""

df

print(df['Channel'].unique())
print(df['Region'].unique())

df['Channel'] = df['Channel'].replace({1: 'Horeca', 2: 'Retail'})
df['Region'] = df['Region'].replace({1: 'Lisbon', 2: 'Oporto', 3: 'Other Region'})

df

df_normcluster['Channel'] = df['Channel']
df_normcluster['Region'] = df['Region']

df.to_excel('df_save.xlsx', index=False)
